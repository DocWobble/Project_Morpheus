# Orpheus-FastAPI Configuration
# Copy this file to .env and customize as needed

# URL for the LLM inference API. The server uses this endpoint to generate tokens.
# Defaults to http://127.0.0.1:1234/v1/completions for local testing.
ORPHEUS_API_URL=http://127.0.0.1:1234/v1/completions

# Timeout in seconds for API requests. Increase if you raise ORPHEUS_MAX_TOKENS or have slow inference.
# Default 120 seconds.
ORPHEUS_API_TIMEOUT=120

# Maximum tokens to generate per request. Higher values allow longer speech but require more resources.
# Default 8192 tokens.
ORPHEUS_MAX_TOKENS=8192

# Sampling temperature controlling creativity of the generated speech.
# Lower values are more deterministic. Default 0.6.
ORPHEUS_TEMPERATURE=0.6

# Nucleus sampling parameter. Only the most probable tokens that sum to this value are considered.
# Default 0.9.
ORPHEUS_TOP_P=0.9

# Repetition penalty is now hardcoded to 1.1 for stability (this setting is no longer used).
# ORPHEUS_REPETITION_PENALTY=1.1

# Output audio sample rate in Hertz. Set to 24000 for best quality.
# Default 24000 Hz.
ORPHEUS_SAMPLE_RATE=24000

# Model name sent to the inference server (e.g., Q2_K, Q4_K_M, or Q8_0 variants).
# Default Orpheus-3b-FT-Q8_0.gguf.
ORPHEUS_MODEL_NAME=Orpheus-3b-FT-Q8_0.gguf

# Port for the web UI and API server.
# Default 5005.
ORPHEUS_PORT=5005

# Host address the web server binds to. 0.0.0.0 makes it accessible on the network.
# Default 0.0.0.0.
ORPHEUS_HOST=0.0.0.0
